{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc77af8d",
   "metadata": {},
   "source": [
    "# ANFIS\n",
    "\n",
    "Experimented with this, but didn't perform well so didn't use it in the final model or report discussion.\n",
    " \n",
    "## Prepare the Data\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6781ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, fbeta_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491f944",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e02d22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1934, 27)\n",
      "(1934,)\n",
      "Index(['num__ADL', 'num__AlcoholConsumption', 'num__CholesterolHDL',\n",
      "       'num__CholesterolLDL', 'num__FunctionalAssessment', 'num__MMSE',\n",
      "       'num__SleepQuality', 'cat__BehavioralProblems_0',\n",
      "       'cat__BehavioralProblems_1', 'cat__CardiovascularDisease_0',\n",
      "       'cat__CardiovascularDisease_1', 'cat__Depression_0',\n",
      "       'cat__Depression_1', 'cat__Diabetes_0', 'cat__Diabetes_1',\n",
      "       'cat__Disorientation_0', 'cat__Disorientation_1',\n",
      "       'cat__EducationLevel_0', 'cat__EducationLevel_1',\n",
      "       'cat__EducationLevel_2', 'cat__EducationLevel_3', 'cat__Hypertension_0',\n",
      "       'cat__Hypertension_1', 'cat__MemoryComplaints_0',\n",
      "       'cat__MemoryComplaints_1', 'cat__Smoking_0', 'cat__Smoking_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Folder path\n",
    "path = Path(\"../data/selected\")\n",
    "\n",
    "# Load data (already split)\n",
    "X_train = pd.read_csv(path / \"X_train_selected.csv\")\n",
    "X_test = pd.read_csv(path / \"X_test_selected.csv\")\n",
    "y_train = pd.read_csv(path / \"y_train.csv\")\n",
    "y_test = pd.read_csv(path / \"y_test.csv\")\n",
    "\n",
    "# Drop the unwanted index-like column\n",
    "if \"Unnamed: 0\" in X_train.columns:\n",
    "    X_train = X_train.drop(columns=[\"Unnamed: 0\"])\n",
    "if \"Unnamed: 0\" in X_test.columns:\n",
    "    X_test = X_test.drop(columns=[\"Unnamed: 0\"])\n",
    "y_train = y_train[\"Diagnosis\"]\n",
    "y_test = y_test[\"Diagnosis\"]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90c9b8",
   "metadata": {},
   "source": [
    "### Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6db8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fis_features = [\n",
    "    \"num__MMSE\",\n",
    "    \"num__FunctionalAssessment\",\n",
    "    \"num__ADL\",\n",
    "]\n",
    "\n",
    "    # \"num__CholesterolHDL\",\n",
    "    # \"num__SystolicBP\",\n",
    "\n",
    "X_train_fis = X_train[fis_features].copy()\n",
    "X_test_fis = X_test[fis_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b1a29",
   "metadata": {},
   "source": [
    "## Prepare the Model\n",
    "\n",
    "DECISIONS:\n",
    "- k = 8\n",
    "- Gaussian\n",
    "- First order consequent\n",
    "- use k-means to develop rule base \n",
    "- HP:\n",
    "    - n_epochs = 30\n",
    "    - lr = 0.01\n",
    "    - Full-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6d533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnfisTS:\n",
    "    \"\"\"\n",
    "    Takagi–Sugeno ANFIS with:\n",
    "      - Gaussian membership functions\n",
    "      - First-order consequents\n",
    "      - K-means rule initialisation\n",
    "      - Hybrid learning: LS for consequents + GD for premises\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_inputs,\n",
    "                 n_rules=8,\n",
    "                 n_epochs=30,\n",
    "                 lr=0.01,\n",
    "                 min_sigma=1e-2,\n",
    "                 random_state=None):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_rules = n_rules\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.min_sigma = min_sigma\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Premise parameters (to be initialised in fit)\n",
    "        self.centers_ = None    # shape (R, d)\n",
    "        self.sigmas_ = None     # shape (R, d)\n",
    "\n",
    "        # Consequent parameters (to be learned)\n",
    "        # shape (R, d+1): [bias, w1, ..., wd] for each rule\n",
    "        self.consequents_ = None\n",
    "\n",
    "        # For monitoring\n",
    "        self.loss_history_ = []\n",
    "\n",
    "    # Internal helpers \n",
    "\n",
    "    def _check_X(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be a 2D array of shape (n_samples, n_features)\")\n",
    "        if X.shape[1] != self.n_inputs:\n",
    "            raise ValueError(\n",
    "                f\"Expected {self.n_inputs} features, got {X.shape[1]}\"\n",
    "            )\n",
    "        return X\n",
    "\n",
    "    def _init_premise_params(self, X):\n",
    "        \"\"\"\n",
    "        Initialise Gaussian MF centres via K-means,\n",
    "        and sigmas via global feature std.\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "\n",
    "        # K-means to get rule centres\n",
    "        km = KMeans(n_clusters=self.n_rules, random_state=self.random_state)\n",
    "        km.fit(X)\n",
    "        centers = km.cluster_centers_  # (R, d)\n",
    "\n",
    "        # Global std per feature; reused for all rules\n",
    "        global_std = X.std(axis=0, ddof=0)\n",
    "        # Avoid zero std\n",
    "        global_std[global_std < self.min_sigma] = self.min_sigma\n",
    "        sigmas = np.tile(global_std, (self.n_rules, 1))  # (R, d)\n",
    "\n",
    "        self.centers_ = centers\n",
    "        self.sigmas_ = sigmas\n",
    "\n",
    "    def _gauss_membership(self, X):\n",
    "        \"\"\"\n",
    "        Compute Gaussian membership values μ_ri(x_ni)\n",
    "        for all samples, rules and features.\n",
    "\n",
    "        Returns:\n",
    "            mu: array, shape (N, R, d)\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "        if self.centers_ is None or self.sigmas_ is None:\n",
    "            raise RuntimeError(\"Premise parameters are not initialised.\")\n",
    "\n",
    "        eps = 1e-6\n",
    "        centers = self.centers_\n",
    "        sigmas = self.sigmas_\n",
    "\n",
    "        N, d = X.shape\n",
    "        R, d2 = centers.shape\n",
    "        assert d == d2\n",
    "\n",
    "        X_exp = X[:, None, :]       # (N, 1, d)\n",
    "        C_exp = centers[None, :, :] # (1, R, d)\n",
    "        S_exp = sigmas[None, :, :]  # (1, R, d)\n",
    "\n",
    "        diff2 = (X_exp - C_exp) ** 2\n",
    "        mu = np.exp(-diff2 / (2.0 * (S_exp ** 2 + eps)))  # (N, R, d)\n",
    "        return mu\n",
    "\n",
    "    def _compute_rule_weights(self, X):\n",
    "        \"\"\"\n",
    "        Compute unnormalised rule firing strengths w_r(x_n)\n",
    "        and their sum S_n for each sample.\n",
    "\n",
    "        Returns:\n",
    "            w: (N, R)\n",
    "            S: (N,)\n",
    "        \"\"\"\n",
    "        mu = self._gauss_membership(X)  # (N, R, d)\n",
    "        w = mu.prod(axis=2)             # product over features → (N, R)\n",
    "        S = w.sum(axis=1) + 1e-6        # (N,), avoid exact zero\n",
    "        return w, S\n",
    "\n",
    "    def _ls_consequents(self, X, y, w, S):\n",
    "        \"\"\"\n",
    "        Least squares estimation of first-order TS consequents.\n",
    "\n",
    "        For each sample n:\n",
    "            y_hat_n = (Σ_r w_rn y_rn) / S_n\n",
    "            where y_rn = a_r0 + Σ_i a_ri x_ni\n",
    "\n",
    "        We rewrite this as a linear regression problem:\n",
    "            z_n = S_n * y_n ≈ Σ_r w_rn y_rn\n",
    "        and regress z on features [w_rn, w_rn x_n1, ..., w_rn x_nd] for all rules.\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "        y = np.asarray(y, dtype=float).ravel()\n",
    "        N, d = X.shape\n",
    "        Nw, R = w.shape\n",
    "        assert N == Nw\n",
    "\n",
    "        # Design matrix\n",
    "        X1 = np.concatenate([np.ones((N, 1)), X], axis=1)  # (N, d+1)\n",
    "        # For each rule r, column block is w_rn * [1, x_1, ..., x_d]\n",
    "        Phi_rule = w[:, :, None] * X1[:, None, :]          # (N, R, d+1)\n",
    "        Phi = Phi_rule.reshape(N, R * (d + 1))             # (N, R*(d+1))\n",
    "\n",
    "        # Target z = S * y\n",
    "        z = S * y\n",
    "\n",
    "        # Solve LS: Phi θ ≈ z\n",
    "        theta, *_ = np.linalg.lstsq(Phi, z, rcond=None)\n",
    "        consequents = theta.reshape(R, d + 1)              # (R, d+1)\n",
    "        return consequents\n",
    "\n",
    "    def _forward_output(self, X):\n",
    "        \"\"\"\n",
    "        Compute ANFIS output ŷ(X) given current parameters.\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "        if self.consequents_ is None:\n",
    "            raise RuntimeError(\"Consequents are not estimated yet. Call fit() first.\")\n",
    "\n",
    "        w, S = self._compute_rule_weights(X)               # (N, R), (N,)\n",
    "        X1 = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)  # (N, d+1)\n",
    "\n",
    "        # y_rn = a_r0 + Σ_i a_ri x_ni\n",
    "        y_rule = X1 @ self.consequents_.T                  # (N, R)\n",
    "\n",
    "        # ŷ_n = Σ_r w_rn y_rn / S_n\n",
    "        y_hat = (w * y_rule).sum(axis=1) / S               # (N,)\n",
    "        return y_hat\n",
    "\n",
    "    def _compute_premise_grads(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute MSE loss and gradients of loss w.r.t. centres and sigmas.\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "        y = np.asarray(y, dtype=float).ravel()\n",
    "        N, d = X.shape\n",
    "        R, d2 = self.centers_.shape\n",
    "        assert d == d2\n",
    "\n",
    "        eps = 1e-6\n",
    "        centers = self.centers_\n",
    "        sigmas = self.sigmas_\n",
    "        consequents = self.consequents_\n",
    "\n",
    "        # Memberships, weights\n",
    "        X_exp = X[:, None, :]       # (N, 1, d)\n",
    "        C_exp = centers[None, :, :] # (1, R, d)\n",
    "        S_exp = sigmas[None, :, :]  # (1, R, d)\n",
    "        diff = X_exp - C_exp        # (N, R, d)\n",
    "        diff2 = diff ** 2\n",
    "\n",
    "        mu = np.exp(-diff2 / (2.0 * (S_exp ** 2 + eps)))  # (N, R, d)\n",
    "        w = mu.prod(axis=2)                               # (N, R)\n",
    "        S = w.sum(axis=1) + eps                           # (N,)\n",
    "\n",
    "        # Forward TS output\n",
    "        X1 = np.concatenate([np.ones((N, 1)), X], axis=1)     # (N, d+1)\n",
    "        y_rule = X1 @ consequents.T                           # (N, R)\n",
    "        y_hat = (w * y_rule).sum(axis=1) / S                  # (N,)\n",
    "\n",
    "        e = y_hat - y\n",
    "        loss = np.mean(e ** 2)\n",
    "\n",
    "        # dL/dy_hat\n",
    "        dL_dyhat = 2.0 * e / N                                # (N,)\n",
    "\n",
    "        # dy_hat/dw_r = (y_r - y_hat) / S\n",
    "        tmp = (y_rule - y_hat[:, None]) / S[:, None]          # (N, R)\n",
    "        dL_dw = dL_dyhat[:, None] * tmp                       # (N, R)\n",
    "\n",
    "        # dL/dmu_ri = dL/dw_r * dw_r/dmu_ri\n",
    "        # w_r = Π_i mu_ri ⇒ dw_r/dmu_ri = w_r / mu_ri\n",
    "        dL_dmu = dL_dw[:, :, None] * w[:, :, None] / (mu + eps)  # (N, R, d)\n",
    "\n",
    "        # dμ/dc_ri = μ * (x_i - c_ri) / σ_ri^2\n",
    "        dmu_dc = mu * (diff / (S_exp ** 2 + eps))\n",
    "        # dμ/dσ_ri = μ * (x_i - c_ri)^2 / σ_ri^3\n",
    "        dmu_dsigma = mu * (diff2 / (S_exp ** 3 + eps))\n",
    "\n",
    "        # Sum over samples n → gradients per rule and feature\n",
    "        dL_dc = (dL_dmu * dmu_dc).sum(axis=0)       # (R, d)\n",
    "        dL_dsigma = (dL_dmu * dmu_dsigma).sum(axis=0)  # (R, d)\n",
    "\n",
    "        return loss, dL_dc, dL_dsigma\n",
    "\n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"\n",
    "        Train ANFIS using hybrid learning:\n",
    "          - K-means initialisation for premises\n",
    "          - Per-epoch LS update for consequents\n",
    "          - Per-epoch gradient descent for premise parameters\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "        y = np.asarray(y, dtype=float).ravel()\n",
    "\n",
    "        # 1. Initialise premise parameters by K-means on X\n",
    "        if self.centers_ is None or self.sigmas_ is None:\n",
    "            self._init_premise_params(X)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # 2. LS step for consequents\n",
    "            w, S = self._compute_rule_weights(X)\n",
    "            self.consequents_ = self._ls_consequents(X, y, w, S)\n",
    "\n",
    "            # 3. Gradient step for premise parameters\n",
    "            loss, grad_c, grad_sigma = self._compute_premise_grads(X, y)\n",
    "            self.loss_history_.append(loss)\n",
    "\n",
    "            # Update centres and sigmas\n",
    "            self.centers_ -= self.lr * grad_c\n",
    "            self.sigmas_ -= self.lr * grad_sigma\n",
    "            # Keep sigmas positive and not too small\n",
    "            self.sigmas_ = np.clip(self.sigmas_, self.min_sigma, None)\n",
    "\n",
    "            # if verbose:\n",
    "                # print(f\"Epoch {epoch+1}/{self.n_epochs} - MSE: {loss:.6f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict continuous ANFIS outputs (risk scores).\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "        return self._forward_output(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4aec6",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e528a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "\n",
    "anfis = AnfisTS(\n",
    "    n_inputs=n_inputs,\n",
    "    n_rules=8,\n",
    "    n_epochs=30,\n",
    "    lr=0.01,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "anfis.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "# Example: get scores\n",
    "y_train_score = anfis.predict(X_train)\n",
    "y_test_score = anfis.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbbed9",
   "metadata": {},
   "source": [
    "## Select Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0f3f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inner shape: (1547, 27)\n",
      "Validation shape: (387, 27)\n",
      "\n",
      "Best threshold on validation: 0.00\n",
      "Validation F2 at best threshold: 0.7072\n",
      "\n",
      "Test ROC AUC (threshold-independent): 0.6416\n",
      "Test F2 at threshold 0.000000: 0.7238\n",
      "\n",
      "Confusion matrix:\n",
      "[[59 80]\n",
      " [11 65]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.843     0.424     0.565       139\n",
      "           1      0.448     0.855     0.588        76\n",
      "\n",
      "    accuracy                          0.577       215\n",
      "   macro avg      0.646     0.640     0.576       215\n",
      "weighted avg      0.703     0.577     0.573       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 80/20 split from your training data\n",
    "X_train_inner, X_val, y_train_inner, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Train inner shape:\", X_train_inner.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "\n",
    "# 2. Train ANFIS on the *inner* training set\n",
    "n_inputs = X_train_inner.shape[1]\n",
    "\n",
    "anfis = AnfisTS(\n",
    "    n_inputs=n_inputs,\n",
    "    n_rules=8,\n",
    "    n_epochs=30,\n",
    "    lr=0.01,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "anfis.fit(X_train_inner, y_train_inner, verbose=True)\n",
    "\n",
    "# 3. Validation and test scores\n",
    "y_val_score = anfis.predict(X_val)\n",
    "y_test_score = anfis.predict(X_test)\n",
    "\n",
    "# 4. Threshold search on validation scores\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "best_f2 = -np.inf\n",
    "best_thresh = None\n",
    "\n",
    "for t in thresholds:\n",
    "    y_val_pred = (y_val_score >= t).astype(int)\n",
    "    f2 = fbeta_score(y_val, y_val_pred, beta=2)\n",
    "    if f2 > best_f2:\n",
    "        best_f2 = f2\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"\\nBest threshold on validation: {best_thresh:.2f}\")\n",
    "print(f\"Validation F2 at best threshold: {best_f2:.4f}\")\n",
    "\n",
    "# 5. Evaluate on test data with chosen threshold\n",
    "test_auc = roc_auc_score(y_test, y_test_score)\n",
    "print(f\"\\nTest ROC AUC (threshold-independent): {test_auc:.4f}\")\n",
    "\n",
    "y_test_pred = (y_test_score >= best_thresh).astype(int)\n",
    "\n",
    "test_f2 = fbeta_score(y_test, y_test_pred, beta=2)\n",
    "print(f\"Test F2 at threshold {best_thresh:.6f}: {test_f2:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
