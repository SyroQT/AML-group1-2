{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd0c401",
   "metadata": {},
   "source": [
    "# Fuzzy Inference System\n",
    "\n",
    "## Takagi-Sugeno (TS) Fuzzy Model\n",
    "We use a Takagi-Sugeno (TS) Fuzzy Model since we want an output probability that the patient has alzheimers, rather than a fuzzy set output as with the Mamdani model. Since the output is binary (AD vs Non-AD) we use zero-order TS.\n",
    "\n",
    "## Preparing Fuzzy Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88ace1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9a78bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simpful import *\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b042e8",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7dbe4c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1934, 11)\n",
      "(1934,)\n",
      "Index(['BehavioralProblems', 'Diabetes', 'EducationLevel', 'MemoryComplaints',\n",
      "       'ADL', 'AlcoholConsumption', 'CholesterolHDL', 'FunctionalAssessment',\n",
      "       'MMSE', 'SleepQuality', 'SystolicBP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Folder path\n",
    "path = Path(\"../data/selected\")\n",
    "\n",
    "# Load data (already split)\n",
    "X_train = pd.read_csv(path / \"X_train_selected_fis.csv\")\n",
    "X_test = pd.read_csv(path / \"X_test_selected_fis.csv\")\n",
    "y_train = pd.read_csv(path / \"y_train.csv\")\n",
    "y_test = pd.read_csv(path / \"y_test.csv\")\n",
    "\n",
    "# Drop the unwanted index-like column\n",
    "if \"Unnamed: 0\" in X_train.columns:\n",
    "    X_train = X_train.drop(columns=[\"Unnamed: 0\"])\n",
    "if \"Unnamed: 0\" in X_test.columns:\n",
    "    X_test = X_test.drop(columns=[\"Unnamed: 0\"])\n",
    "y_train = y_train[\"Diagnosis\"]\n",
    "y_test = y_test[\"Diagnosis\"]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61163b",
   "metadata": {},
   "source": [
    "### Select Features\n",
    "\n",
    "The features used for this model are selected based on the following: from the features identified during the feature engineering, only the numerical ones are considered since the categorical features have too few classes to create meaningful fuzzy sets. Of these 7 numerical features, we selct 5 which provide both cognitive/functional and vascular/metabolic context:\n",
    "- MMSE (Mini-Mental State Examination) - Standard global cognitive screening test used in dementia and Alzheimer’s diagnosis and staging\n",
    "- FunctionalAssessment - Functional decline (managing finances, medication, daily tasks) is core to dementia diagnosis and staging, not just a side measure\n",
    "- ADL (Activities of Daily Living) - ADL scores (bathing, dressing, feeding, toileting, etc.) are a classic way to measure the impact of dementia on basic autonomy\n",
    "- CholesterolHDL - Vascular risk factors (lipids, hypertension, diabetes) are important for vascular dementia and mixed dementia, and they also interact with Alzheimer’s pathology\n",
    "- SystolicBP - Hypertension (especially midlife) is a key risk factor for later-life cognitive impairment and dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "1c14e325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MMSE  FunctionalAssessment       ADL  CholesterolHDL  SystolicBP\n",
      "0  12.292725              6.751583  9.061350       50.421329         104\n",
      "1   3.731915              8.136757  5.886320       51.891374         173\n",
      "2  26.980845              4.803596  0.123688       36.567192         120\n",
      "3   2.313023              2.952020  9.017740       58.584837         149\n",
      "4  19.739526              6.931809  1.927703       52.130322         129\n"
     ]
    }
   ],
   "source": [
    "fis_features = [\n",
    "    \"MMSE\",\n",
    "    \"FunctionalAssessment\",\n",
    "    \"ADL\",\n",
    "    \"CholesterolHDL\",\n",
    "    \"SystolicBP\",\n",
    "]\n",
    "\n",
    "X_train_fis = X_train[fis_features].copy()\n",
    "X_test_fis = X_test[fis_features].copy()\n",
    "print(X_train_fis.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286abc87",
   "metadata": {},
   "source": [
    "### Create Fuzzy Set Membership Functions\n",
    "\n",
    "- MMSE - https://muhc.ca/sites/default/files/micro/m-PT-OT/OT/Mini-Mental-State-Exam-%28MMSE%29.pdf\n",
    "    - 24-30: No cognitive impairment \n",
    "    - 18-23: Mild cognitive impairment \n",
    "    - 0-17: Severe cognitive impairment \n",
    "- Functional Assessment\n",
    "    - Since it isn't standardised, generic\n",
    "- Activities of daily living (ADL)\n",
    "    - Since it isn't standardised, generic\n",
    "- Cholesteroal HDL - https://www.heartuk.org.uk/cholesterol/understanding-your-cholesterol-test-results-\n",
    "    - Threshold differs for men and women:\n",
    "    - above 39 for a man is healthy\n",
    "    - above 46 for a woman is healthy\n",
    "    - But with fuzzy sets we can blur the boundary\n",
    "- Systolic BP - https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings\n",
    "    - \\<120: Normal\n",
    "    - 120-129: Elevated\n",
    "    - 130-139: Stage 1 Hypertension\n",
    "    - 140-180: Stage 2 Hypertension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "92e4a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = FuzzySystem(show_banner=False)\n",
    "\n",
    "# Input: MMSE\n",
    "MMSE_S_1 = FuzzySet(function=Triangular_MF(a=0, b=0, c=19), term=\"Severe\")\n",
    "MMSE_S_2 = FuzzySet(function=Triangular_MF(a=16, b=20, c=25), term=\"Mild\")\n",
    "MMSE_S_3 = FuzzySet(function=Triangular_MF(a=22, b=30, c=30), term=\"Normal\")\n",
    "MMSE = LinguisticVariable([MMSE_S_1, MMSE_S_2, MMSE_S_3], universe_of_discourse=[0,30])\n",
    "FS.add_linguistic_variable(\"MMSE\", MMSE)\n",
    "\n",
    "# Input: Functional Assessment\n",
    "FUNC_S_1 = FuzzySet(function=Triangular_MF(a=0, b=0, c=4), term=\"SevereImpairment\")\n",
    "FUNC_S_2 = FuzzySet(function=Triangular_MF(a=2, b=5, c=8), term=\"ModerateImpairment\")\n",
    "FUNC_S_3 = FuzzySet(function=Triangular_MF(a=6, b=10, c=10), term=\"NoImpairment\")\n",
    "FUNC = LinguisticVariable([FUNC_S_1, FUNC_S_2, FUNC_S_3], universe_of_discourse=[0,10])\n",
    "FS.add_linguistic_variable(\"FunctionalAssessment\", FUNC)\n",
    "\n",
    "# Input: ADL\n",
    "ADL_S_1 = FuzzySet(function=Triangular_MF(a=0, b=0, c=4), term=\"SevereImpairment\")\n",
    "ADL_S_2 = FuzzySet(function=Triangular_MF(a=2, b=5, c=8), term=\"ModerateImpairment\")\n",
    "ADL_S_3 = FuzzySet(function=Triangular_MF(a=6, b=10, c=10), term=\"NoImpairment\")\n",
    "ADL = LinguisticVariable([ADL_S_1, ADL_S_2, ADL_S_3], universe_of_discourse=[0,10])\n",
    "FS.add_linguistic_variable(\"ADL\", ADL)\n",
    "\n",
    "# Input: Cholesterol HDL\n",
    "HDL_S_1 = FuzzySet(function=Triangular_MF(a=20, b=20, c=46), term=\"Low\")\n",
    "HDL_S_2 = FuzzySet(function=Triangular_MF(a=39, b=100, c=100), term=\"Normal\")\n",
    "HDL = LinguisticVariable([HDL_S_1, HDL_S_2], universe_of_discourse=[20,100])\n",
    "FS.add_linguistic_variable(\"CholesterolHDL\", HDL)\n",
    "\n",
    "# Input: Systolic BP\n",
    "SBP_S_1 = FuzzySet(function=Triangular_MF(a=90,  b=90,  c=120), term=\"Normal\")\n",
    "SBP_S_2 = FuzzySet(function=Triangular_MF(a=115, b=125, c=135), term=\"Elevated\")\n",
    "SBP_S_3 = FuzzySet(function=Triangular_MF(a=130, b=160, c=180), term=\"Hypertensive\")\n",
    "SBP = LinguisticVariable([SBP_S_1, SBP_S_2, SBP_S_3], universe_of_discourse=[90,180])\n",
    "FS.add_linguistic_variable(\"SystolicBP\", SBP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79b4a3",
   "metadata": {},
   "source": [
    "## Initialise the Rule Base\n",
    "\n",
    "There are two options for determining the rule base:\n",
    "- Grid rule base - not ideal for this case since there are 162 possible rules (3x3x3x2x3) which is too large for grid\n",
    "- Data-driven rule extraction - works well for this case\n",
    "\n",
    "DECISION: With all 5 features there are too many rule options, so instead we determine the rules based only on the cognitive features and then incorporate the remaining features in the consequents. (This was also found to improve model performance on validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8eef91",
   "metadata": {},
   "source": [
    "### Prepare dataframe and input and output columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = [\n",
    "    \"MMSE\",\n",
    "    \"FunctionalAssessment\",\n",
    "    \"ADL\",\n",
    "    \"CholesterolHDL\",\n",
    "    \"SystolicBP\",\n",
    "]\n",
    "\n",
    "input_features_rules = [\n",
    "    \"MMSE\",\n",
    "    \"FunctionalAssessment\",\n",
    "    \"ADL\",\n",
    "]\n",
    "\n",
    "output_col = \"diagnosis\"\n",
    "\n",
    "df_train = X_train_fis.copy()\n",
    "df_train[output_col] = y_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63d6e7",
   "metadata": {},
   "source": [
    "### Get the linguistic terms for each feature for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a676be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_max_membership_term(FS, var_name, x):\n",
    "    \"\"\"\n",
    "    For a given crisp value x of feature var_name,\n",
    "    return the linguistic term with highest membership degree.\n",
    "    \"\"\"\n",
    "    fuzzy_sets = FS.get_fuzzy_sets(var_name)\n",
    "    best_term = None\n",
    "    best_mu = -1.0\n",
    "\n",
    "    for fs in fuzzy_sets:\n",
    "        term = fs.get_term()\n",
    "        mu = fs.get_value(x)\n",
    "        if mu > best_mu:\n",
    "            best_mu = mu\n",
    "            best_term = term\n",
    "\n",
    "    return best_term, best_mu\n",
    "\n",
    "def compute_max_term_combinations(FS, df, input_vars=input_features):\n",
    "    \"\"\"\n",
    "    For each row in df:\n",
    "      - find the max-membership term for each input variable\n",
    "      - create a tuple combo of these terms\n",
    "    Returns:\n",
    "      df_with_terms: original df with *_term columns and 'combo_key'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    combo_keys = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        combo_terms = []\n",
    "        for var in input_vars:\n",
    "            x = row[var]\n",
    "            term, mu = get_max_membership_term(FS, var, x)\n",
    "            df.loc[idx, f\"{var}_term\"] = term\n",
    "            combo_terms.append(term)\n",
    "\n",
    "        combo_key = tuple(combo_terms)\n",
    "        combo_keys.append(combo_key)\n",
    "\n",
    "    df[\"combo_key\"] = combo_keys\n",
    "    return df\n",
    "\n",
    "# df_terms = compute_max_term_combinations(FS, df_train, input_features_rules)\n",
    "# df_terms.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c37e78",
   "metadata": {},
   "source": [
    "### Aggregate diagnoses for term combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f4626740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_terms[\"combo_key\"].value_counts().head(10)\n",
    "# print(\"Unique combos:\", df_terms[\"combo_key\"].nunique())\n",
    "\n",
    "# df_terms[\n",
    "#     [\n",
    "#         \"MMSE\", \"FunctionalAssessment\", \"ADL\",\n",
    "#         \"MMSE_term\", \"FunctionalAssessment_term\",\n",
    "#         \"ADL_term\", \"combo_key\"\n",
    "#     ]\n",
    "# ].head(10)\n",
    "\n",
    "# df_terms[\n",
    "#     [\n",
    "#         \"MMSE\", \"FunctionalAssessment\", \"ADL\",\n",
    "#         \"CholesterolHDL\", \"SystolicBP\",\n",
    "#         \"MMSE_term\", \"FunctionalAssessment_term\",\n",
    "#         \"ADL_term\", \"CholesterolHDL_term\",\n",
    "#         \"SystolicBP_term\", \"combo_key\"\n",
    "#     ]\n",
    "# ].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e87cc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_combinations(df_terms, input_vars=input_features, label_col=output_col):\n",
    "    \"\"\"\n",
    "    Build a table of:\n",
    "      - each unique combo_key\n",
    "      - total count\n",
    "      - AD / Non-AD counts\n",
    "      - AD_ratio\n",
    "    Returns:\n",
    "      combo_df: data frame with one row per combination\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    # Group by combo_key\n",
    "    grouped = df_terms.groupby(\"combo_key\")\n",
    "    print(len(grouped))\n",
    "\n",
    "    for combo, group in grouped:\n",
    "        total = len(group)\n",
    "        # binary label 0/1 and 1 = AD\n",
    "        ad_count = group[label_col].sum()\n",
    "        nonad_count = total - ad_count\n",
    "        ad_ratio = ad_count / total if total > 0 else 0.0\n",
    "\n",
    "        # unpack terms into separate columns for readability\n",
    "        combo_dict = {\"combo_key\": combo,\n",
    "                      \"total\": total,\n",
    "                      \"AD_count\": ad_count,\n",
    "                      \"NonAD_count\": nonad_count,\n",
    "                      \"AD_ratio\": ad_ratio}\n",
    "\n",
    "        for var, term in zip(input_vars, combo):\n",
    "            combo_dict[f\"{var}_term\"] = term\n",
    "\n",
    "        records.append(combo_dict)\n",
    "\n",
    "    combo_df = pd.DataFrame.from_records(records)\n",
    "    return combo_df\n",
    "\n",
    "# combo_df = aggregate_combinations(df_terms, input_features_rules, output_col)\n",
    "# combo_df.sort_values(\"total\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d771ab18",
   "metadata": {},
   "source": [
    "### Select Rule Antecedents\n",
    "\n",
    "The min_support and min_purity are hyperparameters which determine the number of rules. We optimise these with cross validation during HPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_rule_antecedents(df, input_vars=input_features_rules, min_support=45, min_purity=0.8):\n",
    "    \"\"\"\n",
    "    Select combinations that will become rule antecedents, based on:\n",
    "      - min_support: minimum number of samples for that combination\n",
    "      - min_purity: majority class proportion (e.g. AD_ratio >= 0.7 or <= 0.3)\n",
    "\n",
    "    Returns:\n",
    "      rules: list of dicts like\n",
    "             {'antecedent': { 'MMSE': 'Severe', 'Functional Assessment': 'SevereImpairment', ... },\n",
    "              'total': ...,\n",
    "              'AD_ratio': ...}\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        total = row[\"total\"]\n",
    "        ad_ratio = row[\"AD_ratio\"]\n",
    "\n",
    "        if total < min_support:\n",
    "            continue\n",
    "\n",
    "        # purity: either strongly AD or strongly Non-AD\n",
    "        if (ad_ratio >= min_purity) or (ad_ratio <= 1 - min_purity):\n",
    "            antecedent = {}\n",
    "            for var in input_vars:\n",
    "                antecedent[var] = row[f\"{var}_term\"]\n",
    "\n",
    "            rule_info = {\n",
    "                \"antecedent\": antecedent,\n",
    "                \"total\": int(total),\n",
    "                \"AD_ratio\": float(ad_ratio),\n",
    "            }\n",
    "            rules.append(rule_info)\n",
    "\n",
    "    return rules\n",
    "\n",
    "# rules = select_rule_antecedents(combo_df, input_features_rules)\n",
    "# len(rules), rules[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba59256",
   "metadata": {},
   "source": [
    "### Compute firing strengths for each rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "51f35221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_membership(FS, var_name, term_name, x):\n",
    "    \"\"\"\n",
    "    Return μ_{term_name}(x) for the given variable.\n",
    "    \"\"\"\n",
    "    fuzzy_sets = FS.get_fuzzy_sets(var_name)\n",
    "    for fs in fuzzy_sets:\n",
    "        if fs.get_term() == term_name:\n",
    "            return fs.get_value(x)\n",
    "    raise ValueError(f\"Term '{term_name}' not found for variable '{var_name}'\")\n",
    "\n",
    "def firing_strength_for_rule(FS, rule_antecedent, sample_row):\n",
    "    \"\"\"\n",
    "    Compute the firing strength of ONE rule for ONE sample.\n",
    "    \n",
    "    rule_antecedent: dict like { 'MMSE': 'Severe', 'Functional Assessment': 'SevereImpairment', ... }\n",
    "    sample_row: a pandas Series with crisp input values for the same variables\n",
    "    \"\"\"\n",
    "    strength = 1.0   # product t-norm for fuzzy AND\n",
    "\n",
    "    for var_name, term_name in rule_antecedent.items():\n",
    "        x = sample_row[var_name]\n",
    "        mu = get_membership(FS, var_name, term_name, x)\n",
    "        strength *= mu\n",
    "\n",
    "        # early exit if strength drops to zero\n",
    "        if strength == 0.0:\n",
    "            break\n",
    "\n",
    "    return strength\n",
    "\n",
    "def compute_firing_strength_matrix(FS, rules, X_fis):\n",
    "    \"\"\"\n",
    "    FS    : Simpful FuzzySystem with your 5 input variables.\n",
    "    rules : list of rule dicts, each with an 'antecedent' key as above.\n",
    "    X_fis : DataFrame of raw inputs used for FIS\n",
    "            (columns: 'MMSE', 'Functional Assessment', 'Activities of Daily Living',\n",
    "                      'Cholesterol HDL', 'Systolic BP')\n",
    "    \n",
    "    Returns:\n",
    "        W: numpy array of shape (N_samples, N_rules)\n",
    "    \"\"\"\n",
    "    N = len(X_fis)\n",
    "    R = len(rules)\n",
    "    W = np.zeros((N, R), dtype=float)\n",
    "\n",
    "    for i, (idx, row) in enumerate(X_fis.iterrows()):\n",
    "        for j, rule in enumerate(rules):\n",
    "            antecedent = rule[\"antecedent\"]\n",
    "            W[i, j] = firing_strength_for_rule(FS, antecedent, row)\n",
    "\n",
    "    return W\n",
    "\n",
    "# # X_train_fis must be the RAW data (not scaled) with the FIS column names\n",
    "# W_train = compute_firing_strength_matrix(FS, rules, X_train_fis)\n",
    "\n",
    "# print(W_train.shape)  # (N_samples, 11) for your current ~11 rules\n",
    "# print(W_train[:5])    # first 5 samples' firing strengths\n",
    "\n",
    "# rule_names = [f\"rule_{k}\" for k in range(len(rules))]\n",
    "# W_train_df = pd.DataFrame(W_train, index=X_train_fis.index, columns=rule_names)\n",
    "\n",
    "# row_sums = W_train.sum(axis=1)\n",
    "# covered = np.mean(row_sums > 0)\n",
    "# print(\"Fraction of training samples covered by at least one rule:\", covered)\n",
    "\n",
    "# for j, rule in enumerate(rules):\n",
    "#     nz = np.count_nonzero(W_train[:, j] > 0)\n",
    "#     print(f\"Rule {j}: nonzero activations = {nz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613fe8ac",
   "metadata": {},
   "source": [
    "## Training the TS Model\n",
    "\n",
    "### Fit the TS Consequents\n",
    "\n",
    "DECISION: We opt for linear consequents (rule describes the region, and the linear model describes the effect of variables in that region) since it allows us to include the effect of the features excluded from the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7a96b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ts_linear_consequents(W_train, X_fis, y_train):\n",
    "    \"\"\"\n",
    "    Learn linear TS consequents:\n",
    "        y_r(x) = a_{r0} + a_{r1}*x1 + ... + a_{rd}*xd\n",
    "\n",
    "    using global least squares on the Takagi–Sugeno structure:\n",
    "        y_hat(x) = sum_r phi_r(x) * y_r(x)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W_train : array (N, R)\n",
    "        Firing strengths for each sample and rule.\n",
    "    X_fis : DataFrame (N, d)\n",
    "        Raw input features used in the consequents (here: MMSE, Func, ADL, HDL, SBP).\n",
    "    y_train : array-like (N,)\n",
    "        Binary labels (0/1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    consequents : list of dicts\n",
    "        Each dict has keys 'bias' and 'weights' (length d).\n",
    "    \"\"\"\n",
    "    W = np.asarray(W_train, dtype=float)\n",
    "    X = np.asarray(X_fis.values, dtype=float)\n",
    "    y = np.asarray(y_train, dtype=float)\n",
    "\n",
    "    N, R = W.shape\n",
    "    _, d = X.shape\n",
    "\n",
    "    # 1) Normalise firing strengths -> phi\n",
    "    sum_w = W.sum(axis=1, keepdims=True)  # (N,1)\n",
    "    phi = np.zeros_like(W)\n",
    "    nonzero = sum_w[:, 0] > 0\n",
    "    phi[nonzero, :] = W[nonzero, :] / sum_w[nonzero, :]\n",
    "\n",
    "    # 2) Build design matrix Z (N x (R*(d+1)))\n",
    "    # For each rule r, we have (1, x1,...,xd) scaled by phi_r.\n",
    "    Z = np.zeros((N, R * (d + 1)), dtype=float)\n",
    "\n",
    "    for n in range(N):\n",
    "        for r in range(R):\n",
    "            coeff = phi[n, r]\n",
    "            if coeff == 0.0:\n",
    "                continue\n",
    "            start = r * (d + 1)\n",
    "            Z[n, start] = coeff                # bias term\n",
    "            Z[n, start + 1:start + 1 + d] = coeff * X[n, :]\n",
    "\n",
    "    # 3) Solve least squares: Z @ theta ≈ y\n",
    "    # (you could add regularisation if needed, but plain LS is fine here)\n",
    "    theta, *_ = np.linalg.lstsq(Z, y, rcond=None)\n",
    "\n",
    "    # 4) Unpack into per-rule parameters\n",
    "    consequents = []\n",
    "    for r in range(R):\n",
    "        start = r * (d + 1)\n",
    "        a0 = theta[start]\n",
    "        a = theta[start + 1:start + 1 + d]\n",
    "        consequents.append({\n",
    "            \"bias\": float(a0),\n",
    "            \"weights\": a,   # numpy array of length d\n",
    "        })\n",
    "\n",
    "    return consequents\n",
    "\n",
    "# consequents = fit_ts_linear_consequents(W_train, X_train_fis, y_train)\n",
    "# len(consequents), consequents[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fa11a",
   "metadata": {},
   "source": [
    "### Predict with the trained TS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "de04d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_predict_linear(FS, rules, consequents, X_fis, global_default=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions with a trained linear TS model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    FS : FuzzySystem\n",
    "        Your Simpful fuzzy system with defined input variables and sets.\n",
    "    rules : list of dicts\n",
    "        Rule antecedents as before.\n",
    "    consequents : list of dicts\n",
    "        Output of fit_ts_linear_consequents.\n",
    "    X_fis : DataFrame\n",
    "        Raw input features (same columns/order as used in training).\n",
    "    global_default : float\n",
    "        Fallback prediction if a sample has zero firing strength for all rules.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_hat : array (N,)\n",
    "        Continuous outputs (you can threshold at 0.5 for classification).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X_fis.values, dtype=float)\n",
    "    N = len(X_fis)\n",
    "    R = len(rules)\n",
    "    d = X.shape[1]\n",
    "\n",
    "    y_hat = np.zeros(N, dtype=float)\n",
    "\n",
    "    for i, (idx, row) in enumerate(X_fis.iterrows()):\n",
    "        # 1) Compute firing strengths for this sample\n",
    "        w = np.zeros(R, dtype=float)\n",
    "        for r, rule in enumerate(rules):\n",
    "            antecedent = rule[\"antecedent\"]\n",
    "            w[r] = firing_strength_for_rule(FS, antecedent, row)\n",
    "\n",
    "        sum_w = w.sum()\n",
    "        if sum_w == 0.0:\n",
    "            # No rule fires: fallback to global default (e.g. mean of y_train)\n",
    "            y_hat[i] = global_default\n",
    "            continue\n",
    "\n",
    "        # 2) Normalise to phi\n",
    "        phi = w / sum_w\n",
    "\n",
    "        # 3) Combine rule outputs\n",
    "        out = 0.0\n",
    "        x_i = X[i, :]\n",
    "        for r in range(R):\n",
    "            a0 = consequents[r][\"bias\"]\n",
    "            a = consequents[r][\"weights\"]\n",
    "            y_r = a0 + np.dot(a, x_i)\n",
    "            out += phi[r] * y_r\n",
    "\n",
    "        y_hat[i] = out\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "# # Global default = mean label (good fallback if no rules fire)\n",
    "# global_default = float(np.mean(y_train))\n",
    "\n",
    "# y_train_hat = ts_predict_linear(FS, rules, consequents, X_train_fis,\n",
    "#                                 global_default=global_default)\n",
    "\n",
    "# # For binary classification, threshold at 0.5 (or tune threshold)\n",
    "# y_train_pred = (y_train_hat >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4299e4c9",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation\n",
    "\n",
    "3 hyperparameters:\n",
    "- min_support - the minimum number of samples with the same term combination required for rule base\n",
    "- min_purity - the minimum purity (number of samples with same output value) of a term combination required for rule base\n",
    "- n_rules_max - maximum number of rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "8eb3a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score_fis(\n",
    "    FS,\n",
    "    X_train_full,\n",
    "    y_train,\n",
    "    fis_features,\n",
    "    rule_input_vars,\n",
    "    min_support,\n",
    "    min_purity,\n",
    "    n_rules_max,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute mean ROC AUC over stratified k-fold CV for one FIS hyperparameter setting.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aucs = []\n",
    "\n",
    "    # Make sure y_train is a 1D array / Series\n",
    "    y_train = np.asarray(y_train).ravel()\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train_full, y_train):\n",
    "        # Split into CV train/val\n",
    "        X_tr_full = X_train_full.iloc[train_idx]\n",
    "        X_val_full = X_train_full.iloc[val_idx]\n",
    "        y_tr = y_train[train_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "\n",
    "        # FIS uses the selected 5 continuous inputs\n",
    "        X_tr_fis = X_tr_full[fis_features].copy()\n",
    "        X_val_fis = X_val_full[fis_features].copy()\n",
    "\n",
    "        # Build df_tr with diagnosis for rule mining\n",
    "        df_tr = X_tr_fis.copy()\n",
    "        df_tr[\"diagnosis\"] = y_tr\n",
    "\n",
    "        # 1) Max-membership combos per sample (for rule inputs only)\n",
    "        df_terms_fold = compute_max_term_combinations(FS, df_tr, rule_input_vars)\n",
    "\n",
    "        # 2) Aggregate combinations\n",
    "        combo_df_fold = aggregate_combinations(df_terms_fold, rule_input_vars, \"diagnosis\")\n",
    "\n",
    "        # 3) Select rule antecedents with current min_support / min_purity\n",
    "        rules_raw = select_rule_antecedents(\n",
    "            combo_df_fold,\n",
    "            input_vars=rule_input_vars,\n",
    "            min_support=min_support,\n",
    "            min_purity=min_purity,\n",
    "        )\n",
    "\n",
    "        # If no rules survive, this configuration is useless\n",
    "        if len(rules_raw) == 0:\n",
    "            # Assign a very poor score\n",
    "            return 0.5\n",
    "\n",
    "        # 4) Rank rules by informativeness and keep top n_rules_max\n",
    "        #    score = support * |AD_ratio - 0.5|\n",
    "        rules_sorted = sorted(\n",
    "            rules_raw,\n",
    "            key=lambda r: r[\"total\"] * abs(r[\"AD_ratio\"] - 0.5),\n",
    "            reverse=True,\n",
    "        )\n",
    "        rules = rules_sorted[:n_rules_max]\n",
    "\n",
    "        # 5) Compute firing strengths on CV-train\n",
    "        W_tr = compute_firing_strength_matrix(FS, rules, X_tr_fis)\n",
    "\n",
    "        # 6) Fit TS linear consequents\n",
    "        consequents = fit_ts_linear_consequents(W_tr, X_tr_fis, y_tr)\n",
    "\n",
    "        # 7) Predict scores on CV-val\n",
    "        global_default = float(y_tr.mean())\n",
    "        y_val_scores = ts_predict_linear(FS, rules, consequents, X_val_fis,\n",
    "                                         global_default=global_default)\n",
    "\n",
    "        # 8) ROC AUC on this fold\n",
    "        try:\n",
    "            auc = roc_auc_score(y_val, y_val_scores)\n",
    "        except ValueError:\n",
    "            # In case only one class appears in the fold (unlikely with stratified CV)\n",
    "            auc = 0.5\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return float(np.mean(aucs)) if aucs else 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 30, 'min_purity': 0.75, 'n_rules_max': 6} → mean CV ROC AUC: 0.7489200515242593\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 30, 'min_purity': 0.75, 'n_rules_max': 8} → mean CV ROC AUC: 0.76524259338772\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 30, 'min_purity': 0.8, 'n_rules_max': 6} → mean CV ROC AUC: 0.748897745813654\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 30, 'min_purity': 0.8, 'n_rules_max': 8} → mean CV ROC AUC: 0.7679331902103907\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 30, 'min_purity': 0.85, 'n_rules_max': 6} → mean CV ROC AUC: 0.7520290253327607\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 30, 'min_purity': 0.85, 'n_rules_max': 8} → mean CV ROC AUC: 0.7658146414770288\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 35, 'min_purity': 0.75, 'n_rules_max': 6} → mean CV ROC AUC: 0.7489200515242593\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 35, 'min_purity': 0.75, 'n_rules_max': 8} → mean CV ROC AUC: 0.7659958780592528\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 35, 'min_purity': 0.8, 'n_rules_max': 6} → mean CV ROC AUC: 0.7483897166165736\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 35, 'min_purity': 0.8, 'n_rules_max': 8} → mean CV ROC AUC: 0.7594731859167025\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 35, 'min_purity': 0.85, 'n_rules_max': 6} → mean CV ROC AUC: 0.7504115070845856\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 35, 'min_purity': 0.85, 'n_rules_max': 8} → mean CV ROC AUC: 0.7509078574495491\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 40, 'min_purity': 0.75, 'n_rules_max': 6} → mean CV ROC AUC: 0.7506076427651353\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 40, 'min_purity': 0.75, 'n_rules_max': 8} → mean CV ROC AUC: 0.7614280807213396\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 40, 'min_purity': 0.8, 'n_rules_max': 6} → mean CV ROC AUC: 0.7447634392443109\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 40, 'min_purity': 0.8, 'n_rules_max': 8} → mean CV ROC AUC: 0.74603935165307\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 40, 'min_purity': 0.85, 'n_rules_max': 6} → mean CV ROC AUC: 0.7310877629884069\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "{'min_support': 40, 'min_purity': 0.85, 'n_rules_max': 8} → mean CV ROC AUC: 0.7310877629884069\n",
      "\n",
      "Best params: {'min_support': 30, 'min_purity': 0.8, 'n_rules_max': 8} with mean CV ROC AUC: 0.7679331902103907\n"
     ]
    }
   ],
   "source": [
    "# Full training data that FIS sees\n",
    "X_train_full = X_train_fis.copy()   \n",
    "y_train_full = y_train[\"diagnosis\"] if isinstance(y_train, pd.DataFrame) else y_train\n",
    "\n",
    "# Antecedent variables\n",
    "rule_input_vars = input_features_rules \n",
    "\n",
    "# Hyperparameter grid (≤ 20 combos)\n",
    "param_grid = {\n",
    "    'min_support': [30, 35, 40],\n",
    "    'min_purity': [0.75, 0.8, 0.85],\n",
    "    'n_rules_max': [6, 8]\n",
    "}\n",
    "\n",
    "param_combos = [\n",
    "    {\"min_support\": ms, \"min_purity\": mp, \"n_rules_max\": nr}\n",
    "    for ms, mp, nr in product(\n",
    "        param_grid[\"min_support\"],\n",
    "        param_grid[\"min_purity\"],\n",
    "        param_grid[\"n_rules_max\"],\n",
    "    )\n",
    "]\n",
    "\n",
    "best_auc = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in param_combos:\n",
    "    auc = cv_score_fis(\n",
    "        FS=FS,\n",
    "        X_train_full=X_train_full,\n",
    "        y_train=y_train_full,\n",
    "        fis_features=fis_features,\n",
    "        rule_input_vars=rule_input_vars,\n",
    "        min_support=params[\"min_support\"],\n",
    "        min_purity=params[\"min_purity\"],\n",
    "        n_rules_max=params[\"n_rules_max\"],\n",
    "        n_splits=5,\n",
    "        random_state=42,\n",
    "    )\n",
    "    print(params, \"→ mean CV ROC AUC:\", auc)\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest params:\", best_params, \"with mean CV ROC AUC:\", best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f32774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "Number of final rules: 8\n",
      "{'antecedent': {'MMSE': 'Severe', 'FunctionalAssessment': 'SevereImpairment', 'ADL': 'SevereImpairment'}, 'total': 107, 'AD_ratio': 0.9626168224299065}\n",
      "{'antecedent': {'MMSE': 'Severe', 'FunctionalAssessment': 'NoImpairment', 'ADL': 'ModerateImpairment'}, 'total': 146, 'AD_ratio': 0.1917808219178082}\n",
      "{'antecedent': {'MMSE': 'Normal', 'FunctionalAssessment': 'ModerateImpairment', 'ADL': 'ModerateImpairment'}, 'total': 76, 'AD_ratio': 0.039473684210526314}\n",
      "{'antecedent': {'MMSE': 'Severe', 'FunctionalAssessment': 'NoImpairment', 'ADL': 'NoImpairment'}, 'total': 87, 'AD_ratio': 0.11494252873563218}\n",
      "{'antecedent': {'MMSE': 'Mild', 'FunctionalAssessment': 'NoImpairment', 'ADL': 'NoImpairment'}, 'total': 60, 'AD_ratio': 0.11666666666666667}\n",
      "{'antecedent': {'MMSE': 'Normal', 'FunctionalAssessment': 'SevereImpairment', 'ADL': 'ModerateImpairment'}, 'total': 46, 'AD_ratio': 0.043478260869565216}\n",
      "{'antecedent': {'MMSE': 'Normal', 'FunctionalAssessment': 'ModerateImpairment', 'ADL': 'SevereImpairment'}, 'total': 46, 'AD_ratio': 0.06521739130434782}\n",
      "{'antecedent': {'MMSE': 'Normal', 'FunctionalAssessment': 'NoImpairment', 'ADL': 'ModerateImpairment'}, 'total': 40, 'AD_ratio': 0.025}\n",
      "Final FIS TRAIN ROC AUC: 0.7944725146198831\n"
     ]
    }
   ],
   "source": [
    "# Final FIS training on full training set using best hyperparameters\n",
    "\n",
    "min_support = best_params[\"min_support\"]\n",
    "min_purity = best_params[\"min_purity\"]\n",
    "n_rules_max = best_params[\"n_rules_max\"]\n",
    "\n",
    "# Build df_terms on full train\n",
    "df_train_full = X_train_full.copy()\n",
    "df_train_full[\"diagnosis\"] = y_train_full\n",
    "\n",
    "df_terms_full = compute_max_term_combinations(FS, df_train_full, rule_input_vars)\n",
    "combo_df_full = aggregate_combinations(df_terms_full, rule_input_vars, \"diagnosis\")\n",
    "\n",
    "# Raw rules from full train\n",
    "rules_raw_full = select_rule_antecedents(\n",
    "    combo_df_full,\n",
    "    input_vars=rule_input_vars,\n",
    "    min_support=min_support,\n",
    "    min_purity=min_purity,\n",
    ")\n",
    "\n",
    "# Rank and keep top n_rules_max\n",
    "rules_sorted_full = sorted(\n",
    "    rules_raw_full,\n",
    "    key=lambda r: r[\"total\"] * abs(r[\"AD_ratio\"] - 0.5),\n",
    "    reverse=True,\n",
    ")\n",
    "rules_final = rules_sorted_full[:n_rules_max]\n",
    "\n",
    "print(\"Number of final rules:\", len(rules_final))\n",
    "for r in rules_final:\n",
    "    print(r)\n",
    "\n",
    "# Firing strengths and consequents on full train\n",
    "W_train_final = compute_firing_strength_matrix(FS, rules_final, X_train_full)\n",
    "consequents_final = fit_ts_linear_consequents(W_train_final, X_train_full, y_train_full)\n",
    "\n",
    "# Evaluate on TRAIN (just to see)\n",
    "global_default_final = float(np.mean(y_train_full))\n",
    "y_train_score = ts_predict_linear(\n",
    "    FS, rules_final, consequents_final, X_train_full, global_default=global_default_final\n",
    ")\n",
    "train_auc = roc_auc_score(y_train_full, y_train_score)\n",
    "print(\"Final FIS TRAIN ROC AUC:\", train_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89efcb",
   "metadata": {},
   "source": [
    "## Threshold optimisation\n",
    "\n",
    "DECISION: Use F2 score to optimise threshold since it prioritises recall while balancing with precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold on validation set (F2): 0.190\n",
      "Validation F2 at best threshold: 0.783\n",
      "Validation accuracy at best threshold: 0.566\n"
     ]
    }
   ],
   "source": [
    "# Validation split and threshold optimisation (using F2)\n",
    "X_train_inner, X_val, y_train_inner, y_val = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train_full,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_val_fis = X_val[fis_features].copy()\n",
    "y_val_scores = ts_predict_linear(\n",
    "    FS, rules_final, consequents_final, X_val_fis, global_default=global_default_final\n",
    ")\n",
    "\n",
    "threshold_grid = np.linspace(0.0, 1.0, 101)\n",
    "\n",
    "best_thresh = 0.5\n",
    "best_f2 = -1.0\n",
    "best_acc = -1.0\n",
    "\n",
    "for thr in threshold_grid:\n",
    "    y_val_pred = (y_val_scores >= thr).astype(int)\n",
    "\n",
    "    f2 = fbeta_score(y_val, y_val_pred, beta=2)\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    if f2 > best_f2:\n",
    "        best_f2 = f2\n",
    "        best_acc = acc\n",
    "        best_thresh = thr\n",
    "\n",
    "print(f\"Best threshold on validation set (F2): {best_thresh:.3f}\")\n",
    "print(f\"Validation F2 at best threshold: {best_f2:.3f}\")\n",
    "print(f\"Validation accuracy at best threshold: {best_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f03f255",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde56e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final FIS TEST ROC AUC: 0.8079326012873911\n",
      "\n",
      "Test metrics at threshold = 0.190\n",
      "Confusion matrix:\n",
      "[[57 82]\n",
      " [ 3 73]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.950     0.410     0.573       139\n",
      "           1      0.471     0.961     0.632        76\n",
      "\n",
      "    accuracy                          0.605       215\n",
      "   macro avg      0.710     0.685     0.602       215\n",
      "weighted avg      0.781     0.605     0.594       215\n",
      "\n",
      "Test F2 at threshold = 0.190: 0.795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test evaluation\n",
    "\n",
    "X_test_fis = X_test[fis_features].copy()\n",
    "y_test_full = y_test[\"diagnosis\"] if isinstance(y_test, pd.DataFrame) else y_test\n",
    "\n",
    "y_test_score = ts_predict_linear(\n",
    "    FS, rules_final, consequents_final, X_test_fis, global_default=global_default_final\n",
    ")\n",
    "test_auc = roc_auc_score(y_test_full, y_test_score)\n",
    "print(\"Final FIS TEST ROC AUC:\", test_auc)\n",
    "\n",
    "# Apply the optimised threshold from the validation set\n",
    "y_test_pred = (y_test_score >= best_thresh).astype(int)\n",
    "\n",
    "print(f\"\\nTest metrics at threshold = {best_thresh:.3f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test_full, y_test_pred))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test_full, y_test_pred, digits=3))\n",
    "\n",
    "test_f2 = fbeta_score(y_test_full, y_test_pred, beta=2)\n",
    "print(f\"Test F2 at threshold = {best_thresh:.3f}: {test_f2:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
